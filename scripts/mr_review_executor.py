#!/usr/bin/env python3
"""
MR Reviewä»»åŠ¡æ‰§è¡Œå™¨å’Œç»“æœèšåˆå™¨
æ‰§è¡Œæ‹†åˆ†åçš„reviewå­ä»»åŠ¡ï¼Œå¹¶èšåˆæ‰€æœ‰findings
"""
import sys
import os
import subprocess
import json
import tempfile
from pathlib import Path
from typing import List, Dict
from task_framework import TaskPlan, SubTask, TaskExecutor, TaskAggregator


def get_diff_for_files(base_branch: str, head_branch: str, file_patterns: List[str]) -> str:
    """è·å–æŒ‡å®šæ–‡ä»¶çš„diff"""
    if not file_patterns:
        # æ²¡æœ‰æŒ‡å®šæ–‡ä»¶ï¼Œè¿”å›å…¨éƒ¨diff
        result = subprocess.run(
            ['git', 'diff', f'{base_branch}...{head_branch}'],
            capture_output=True,
            text=True,
            check=True
        )
        return result.stdout
    
    # è·å–æŒ‡å®šæ–‡ä»¶çš„diff
    cmd = ['git', 'diff', f'{base_branch}...{head_branch}', '--'] + file_patterns
    result = subprocess.run(
        cmd,
        capture_output=True,
        text=True,
        check=True
    )
    return result.stdout


def execute_review_with_copilot(
    subtask: SubTask,
    diff_content: str,
    prompt_template_path: Path,
    workspace: Path
) -> Dict:
    """
    ä½¿ç”¨Copilotæ‰§è¡Œä»£ç å®¡æŸ¥
    
    Args:
        subtask: å­ä»»åŠ¡ä¿¡æ¯
        diff_content: diffå†…å®¹
        prompt_template_path: promptæ¨¡æ¿æ–‡ä»¶è·¯å¾„
        workspace: å·¥ä½œç›®å½•
    
    Returns:
        åŒ…å«findingsçš„å­—å…¸
    """
    print(f"[INFO] Executing review for subtask: {subtask.id}")
    print(f"[INFO] Reviewing {len(subtask.file_patterns)} files")
    print(f"[INFO] Diff size: {len(diff_content)} bytes")
    
    # è¯»å–promptæ¨¡æ¿
    if not prompt_template_path.exists():
        raise FileNotFoundError(f"Prompt template not found: {prompt_template_path}")
    
    prompt_template = prompt_template_path.read_text(encoding='utf-8')
    
    # æ£€æŸ¥æ˜¯å¦æœ‰é¡¹ç›®ç†è§£ä¸Šä¸‹æ–‡
    project_context = ""
    context_file = workspace.parent / ".copilot" / "project_context.md"
    if context_file.exists():
        print(f"[INFO] Loading project context from {context_file}")
        project_context = context_file.read_text(encoding='utf-8')
    
    # æ›¿æ¢å˜é‡
    prompt = prompt_template.replace('{code_diff}', diff_content)
    prompt = prompt.replace('{changed_files}', ', '.join(subtask.file_patterns))
    prompt = prompt.replace('{mr_title}', subtask.title)
    prompt = prompt.replace('{mr_description}', subtask.description)
    
    # å¦‚æœæœ‰é¡¹ç›®ä¸Šä¸‹æ–‡ï¼Œæ·»åŠ åˆ° prompt å¼€å¤´
    if project_context:
        prompt = f"""## Project Context (Generated by AI Analysis)

The following project understanding context was generated by multi-agent analysis:

{project_context}

---

{prompt}"""
        print(f"[INFO] Added project context ({len(project_context)} chars) to prompt")
    
    # ä¸ºæ­¤å­ä»»åŠ¡åˆ›å»ºä¸´æ—¶å·¥ä½œç›®å½•
    subtask_workspace = workspace / f"subtask_{subtask.id}"
    subtask_workspace.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜prompt
    prompt_file = subtask_workspace / "review_prompt.txt"
    prompt_file.write_text(prompt, encoding='utf-8')
    
    # è°ƒç”¨Copilot (ä½¿ç”¨æ ‡å‡†è¾“å…¥é¿å…å‚æ•°é•¿åº¦é™åˆ¶)
    print(f"[INFO] Calling Copilot CLI for review...")
    print(f"[INFO] Prompt size: {len(prompt)} chars, {len(prompt.encode('utf-8'))} bytes")
    try:
        result = subprocess.run(
            ['copilot', '--allow-all-tools'],
            input=prompt,
            capture_output=True,
            text=True,
            timeout=subtask.estimated_time_seconds,
            cwd=subtask_workspace
        )
        
        # ä¿å­˜åŸå§‹è¾“å‡º
        (subtask_workspace / "copilot_raw.txt").write_text(result.stdout, encoding='utf-8')
        
        if result.returncode != 0:
            print(f"[WARN] Copilot returned non-zero exit code: {result.returncode}")
            print(f"[WARN] stderr: {result.stderr[:500]}")
        
    except subprocess.TimeoutExpired:
        print(f"[ERROR] Copilot timeout after {subtask.estimated_time_seconds}s")
        return {
            'status': 'timeout',
            'error': f'Timeout after {subtask.estimated_time_seconds}s',
            'findings': []
        }
    except Exception as e:
        print(f"[ERROR] Failed to execute Copilot: {e}")
        return {
            'status': 'error',
            'error': str(e),
            'findings': []
        }
    
    # æŸ¥æ‰¾ç”Ÿæˆçš„review_findings.json
    findings_file = subtask_workspace / "review_findings.json"
    
    if not findings_file.exists():
        print(f"[WARN] review_findings.json not found for subtask {subtask.id}")
        print(f"[WARN] Copilot may not have generated findings")
        
        # å°è¯•ä»åŸå§‹è¾“å‡ºä¸­è§£æ
        # (è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„è§£æé€»è¾‘)
        
        return {
            'status': 'no_findings',
            'message': 'No findings file generated',
            'findings': []
        }
    
    # è¯»å–findings
    try:
        findings_data = json.loads(findings_file.read_text(encoding='utf-8'))
        
        # ä¸ºæ¯ä¸ªfindingæ·»åŠ subtaskä¿¡æ¯
        findings = findings_data.get('findings', [])
        for finding in findings:
            finding['subtask_id'] = subtask.id
            finding['subtask_category'] = subtask.metadata.get('category', 'unknown')
        
        print(f"[SUCCESS] Found {len(findings)} issues in subtask {subtask.id}")
        
        return {
            'status': 'success',
            'summary': findings_data.get('summary', ''),
            'recommendation': findings_data.get('recommendation', 'NEEDS_DISCUSSION'),
            'findings_count': len(findings),
            'findings': findings,
            'findings_file': str(findings_file)
        }
        
    except json.JSONDecodeError as e:
        print(f"[ERROR] Invalid JSON in findings file: {e}")
        return {
            'status': 'invalid_json',
            'error': str(e),
            'findings': []
        }


def create_review_handler(plan: TaskPlan, workspace: Path):
    """åˆ›å»ºreviewä»»åŠ¡å¤„ç†å™¨"""
    
    def handle_review_task(subtask: SubTask) -> Dict:
        """å¤„ç†å•ä¸ªreviewå­ä»»åŠ¡"""
        base_branch = plan.metadata.get('base_branch', 'main')
        head_branch = plan.metadata.get('head_branch', 'HEAD')
        
        # è·å–æ­¤å­ä»»åŠ¡çš„diff
        diff_content = get_diff_for_files(base_branch, head_branch, subtask.file_patterns)
        
        if not diff_content.strip():
            return {
                'status': 'no_changes',
                'message': 'No changes in specified files',
                'findings': []
            }
        
        # æ£€æŸ¥diffå¤§å°
        diff_size = len(diff_content.encode('utf-8'))
        if diff_size > subtask.max_diff_size_bytes:
            print(f"[WARN] Diff size ({diff_size}) exceeds limit ({subtask.max_diff_size_bytes})")
            print(f"[WARN] Truncating diff...")
            diff_content = diff_content[:subtask.max_diff_size_bytes]
        
        # ç¡®å®špromptæ¨¡æ¿è·¯å¾„
        lang = os.getenv('COPILOT_LANGUAGE', 'zh')
        prompt_path = Path(__file__).parent.parent / f'prompts/{lang}/code_review.txt'
        
        if not prompt_path.exists():
            print(f"[WARN] Prompt not found for language {lang}, falling back to English")
            prompt_path = Path(__file__).parent.parent / 'prompts/en/code_review.txt'
        
        # æ‰§è¡Œreview
        return execute_review_with_copilot(
            subtask=subtask,
            diff_content=diff_content,
            prompt_template_path=prompt_path,
            workspace=workspace
        )
    
    return handle_review_task


def aggregate_review_results(subtasks: List[SubTask]) -> Dict:
    """èšåˆreviewç»“æœ"""
    all_findings = []
    stats = {
        'critical': 0,
        'major': 0,
        'minor': 0,
        'suggestion': 0
    }
    
    total_files_reviewed = set()
    
    for subtask in subtasks:
        if not subtask.result or subtask.result.get('status') != 'success':
            continue
        
        findings = subtask.result.get('findings', [])
        all_findings.extend(findings)
        
        # ç»Ÿè®¡severity
        for finding in findings:
            severity = finding.get('severity', 'minor')
            if severity in stats:
                stats[severity] += 1
        
        # ç»Ÿè®¡å®¡æŸ¥çš„æ–‡ä»¶
        total_files_reviewed.update(subtask.file_patterns)
    
    # ç¡®å®šæ€»ä½“å»ºè®®
    if stats['critical'] > 0:
        recommendation = 'REQUEST_CHANGES'
    elif stats['major'] > 5:
        recommendation = 'REQUEST_CHANGES'
    elif stats['major'] > 0 or stats['minor'] > 10:
        recommendation = 'NEEDS_DISCUSSION'
    else:
        recommendation = 'APPROVE'
    
    return {
        'total_findings': len(all_findings),
        'statistics': stats,
        'recommendation': recommendation,
        'files_reviewed': len(total_files_reviewed),
        'subtasks_completed': len([st for st in subtasks if st.status.value == 'completed']),
        'subtasks_failed': len([st for st in subtasks if st.status.value == 'failed']),
        'findings': all_findings  # åŒ…å«æ‰€æœ‰findingsç”¨äºç”Ÿæˆsummary
    }


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Execute MR Review Task Plan')
    parser.add_argument('--plan', required=True, help='Path to task_plan.json')
    parser.add_argument('--workspace', default='.', help='Working directory')
    parser.add_argument('--output', default='review_results.json', help='Output file')
    parser.add_argument('--summary-output', default='review_summary.md', help='Summary markdown file')
    
    args = parser.parse_args()
    
    try:
        # åŠ è½½è®¡åˆ’
        plan = TaskPlan.load(Path(args.plan))
        workspace = Path(args.workspace).absolute()
        
        print(f"[INFO] Loaded plan: {plan.task_id}")
        print(f"[INFO] Workspace: {workspace}")
        
        # åˆ›å»ºä»»åŠ¡å¤„ç†å™¨
        task_handlers = {
            'review': create_review_handler(plan, workspace),
            'default': lambda st: {'status': 'skipped', 'message': 'No handler for this task type'}
        }
        
        # æ‰§è¡Œä»»åŠ¡
        with TaskExecutor(plan, task_handlers) as executor:
            execution_result = executor.execute()
        
        # èšåˆç»“æœ
        aggregator = TaskAggregator(plan, execution_result)
        aggregators = {
            'review': aggregate_review_results,
            'default': lambda subtasks: {'subtask_count': len(subtasks)}
        }
        
        aggregated = aggregator.aggregate(aggregators)
        
        # ä¿å­˜ç»“æœ
        output_path = Path(args.output)
        output_path.write_text(json.dumps(aggregated, indent=2), encoding='utf-8')
        print(f"\n[SUCCESS] Results saved to {output_path}")
        
        # ç”Ÿæˆæ‘˜è¦
        summary_md = aggregator.generate_summary_markdown(aggregated)
        
        # æ·»åŠ reviewç‰¹å®šçš„è¯¦ç»†ä¿¡æ¯
        if 'review' in aggregated.get('results_by_type', {}):
            review_results = aggregated['results_by_type']['review']
            summary_md += f"""
### ä»£ç å®¡æŸ¥è¯¦æƒ…

**æ€»ä½“å»ºè®®**: **{review_results['recommendation']}**

**å‘ç°çš„é—®é¢˜**:
- ğŸ”´ Critical: {review_results['statistics']['critical']}
- ğŸŸ  Major: {review_results['statistics']['major']}
- ğŸŸ¡ Minor: {review_results['statistics']['minor']}
- ğŸ’¡ Suggestions: {review_results['statistics']['suggestion']}

**å®¡æŸ¥è¦†ç›–**:
- ğŸ“ å®¡æŸ¥æ–‡ä»¶æ•°: {review_results['files_reviewed']}
- âœ… å®Œæˆçš„å­ä»»åŠ¡: {review_results['subtasks_completed']}
- âŒ å¤±è´¥çš„å­ä»»åŠ¡: {review_results['subtasks_failed']}

"""
            
            # æ·»åŠ é«˜ä¼˜å…ˆçº§é—®é¢˜
            critical_major = [f for f in review_results['findings'] 
                            if f.get('severity') in ['critical', 'major']]
            
            if critical_major:
                summary_md += "### âš ï¸ å…³é”®é—®é¢˜\n\n"
                for finding in critical_major[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                    emoji = "ğŸ”´" if finding['severity'] == 'critical' else "ğŸŸ "
                    summary_md += f"{emoji} **{finding.get('title', 'Issue')}**\n"
                    summary_md += f"   - æ–‡ä»¶: `{finding.get('file', 'unknown')}:{finding.get('line', '?')}`\n"
                    summary_md += f"   - {finding.get('description', '')}\n\n"
        
        summary_path = Path(args.summary_output)
        summary_path.write_text(summary_md, encoding='utf-8')
        print(f"[SUCCESS] Summary saved to {summary_path}")
        
        return 0
        
    except Exception as e:
        print(f"[ERROR] Execution failed: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        return 1


if __name__ == '__main__':
    sys.exit(main())
