#!/usr/bin/env python3
"""
é€šç”¨ä»»åŠ¡ç¼–æ’æ¡†æ¶
æ”¯æŒä»»åŠ¡è§„åˆ’ã€æ‹†åˆ†ã€å¹¶è¡Œæ‰§è¡Œå’Œç»“æœèšåˆ
"""
from dataclasses import dataclass, field, asdict
from typing import List, Dict, Optional, Callable, Any
from enum import Enum
from pathlib import Path
import json
import time
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed

logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')
logger = logging.getLogger(__name__)


class TaskStatus(Enum):
    """ä»»åŠ¡çŠ¶æ€"""
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    SKIPPED = "skipped"


class TaskType(Enum):
    """ä»»åŠ¡ç±»å‹"""
    MR_REVIEW = "mr_review"
    ISSUE_IMPLEMENT = "issue_implement"
    CODE_REFACTOR = "code_refactor"
    CUSTOM = "custom"


@dataclass
class SubTask:
    """å­ä»»åŠ¡å®šä¹‰"""
    id: str
    title: str
    description: str
    
    # ä»»åŠ¡é…ç½®
    task_type: str = "generic"  # review, code, test, doc, etc.
    priority: int = 5  # 1-10, 10æœ€é«˜
    
    # èµ„æºé™åˆ¶
    estimated_tokens: int = 5000
    estimated_time_seconds: int = 300
    max_diff_size_bytes: int = 100 * 1024  # 100KB
    
    # ä¾èµ–å…³ç³»
    depends_on: List[str] = field(default_factory=list)
    
    # æ‰§è¡ŒèŒƒå›´ï¼ˆç”¨äºè¿‡æ»¤ï¼‰
    file_patterns: List[str] = field(default_factory=list)
    exclude_patterns: List[str] = field(default_factory=list)
    
    # æ‰§è¡ŒçŠ¶æ€
    status: TaskStatus = TaskStatus.PENDING
    start_time: Optional[float] = None
    end_time: Optional[float] = None
    result: Optional[Dict[str, Any]] = None
    error: Optional[str] = None
    
    # è‡ªå®šä¹‰æ•°æ®
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> Dict:
        """è½¬æ¢ä¸ºå­—å…¸"""
        d = asdict(self)
        d['status'] = self.status.value
        return d
    
    @classmethod
    def from_dict(cls, data: Dict) -> 'SubTask':
        """ä»å­—å…¸åˆ›å»º"""
        data = data.copy()
        if 'status' in data:
            data['status'] = TaskStatus(data['status'])
        return cls(**data)


@dataclass
class TaskPlan:
    """ä»»åŠ¡æ‰§è¡Œè®¡åˆ’"""
    task_id: str
    task_type: str
    title: str
    description: str
    
    # å­ä»»åŠ¡åˆ—è¡¨
    subtasks: List[SubTask] = field(default_factory=list)
    
    # å…¨å±€é…ç½®
    max_total_tokens: int = 150000
    max_execution_time_seconds: int = 3600
    max_concurrent_tasks: int = 3
    batch_size: int = 5
    
    # æ‰§è¡Œç­–ç•¥
    enable_parallel: bool = True
    fail_fast: bool = False  # True: ä»»æ„å¤±è´¥åˆ™åœæ­¢ï¼›False: ç»§ç»­æ‰§è¡Œå…¶ä»–ä»»åŠ¡
    
    # å…ƒæ•°æ®
    created_at: float = field(default_factory=time.time)
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def to_json(self, indent: int = 2) -> str:
        """åºåˆ—åŒ–ä¸ºJSON"""
        return json.dumps({
            'task_id': self.task_id,
            'task_type': self.task_type,
            'title': self.title,
            'description': self.description,
            'subtasks': [st.to_dict() for st in self.subtasks],
            'max_total_tokens': self.max_total_tokens,
            'max_execution_time_seconds': self.max_execution_time_seconds,
            'max_concurrent_tasks': self.max_concurrent_tasks,
            'batch_size': self.batch_size,
            'enable_parallel': self.enable_parallel,
            'fail_fast': self.fail_fast,
            'created_at': self.created_at,
            'metadata': self.metadata
        }, indent=indent)
    
    @classmethod
    def from_json(cls, json_str: str) -> 'TaskPlan':
        """ä»JSONååºåˆ—åŒ–"""
        data = json.loads(json_str)
        subtasks = [SubTask.from_dict(st) for st in data.pop('subtasks', [])]
        return cls(subtasks=subtasks, **data)
    
    def save(self, path: Path) -> None:
        """ä¿å­˜åˆ°æ–‡ä»¶"""
        path.write_text(self.to_json(), encoding='utf-8')
        logger.info(f"Task plan saved to {path}")
    
    @classmethod
    def load(cls, path: Path) -> 'TaskPlan':
        """ä»æ–‡ä»¶åŠ è½½"""
        logger.info(f"Loading task plan from {path}")
        return cls.from_json(path.read_text(encoding='utf-8'))


class TaskExecutor:
    """ä»»åŠ¡æ‰§è¡Œå™¨ - æ”¯æŒä¾èµ–ç®¡ç†å’Œå¹¶è¡Œæ‰§è¡Œ"""
    
    def __init__(
        self,
        plan: TaskPlan,
        task_handlers: Dict[str, Callable[[SubTask], Dict]]
    ):
        """
        åˆå§‹åŒ–æ‰§è¡Œå™¨
        
        Args:
            plan: ä»»åŠ¡è®¡åˆ’
            task_handlers: ä»»åŠ¡ç±»å‹åˆ°å¤„ç†å‡½æ•°çš„æ˜ å°„
                           ä¾‹å¦‚: {'review': handle_review_task, 'code': handle_code_task}
        """
        self.plan = plan
        self.task_handlers = task_handlers
        self.executor = ThreadPoolExecutor(max_workers=plan.max_concurrent_tasks)
        
    def can_execute(self, subtask: SubTask) -> bool:
        """æ£€æŸ¥å­ä»»åŠ¡æ˜¯å¦å¯ä»¥æ‰§è¡Œï¼ˆä¾èµ–å·²å®Œæˆï¼‰"""
        if subtask.status != TaskStatus.PENDING:
            return False
        
        for dep_id in subtask.depends_on:
            dep_task = self._find_subtask(dep_id)
            if not dep_task:
                logger.warning(f"Dependency {dep_id} not found for task {subtask.id}")
                return False
            if dep_task.status != TaskStatus.COMPLETED:
                return False
        return True
    
    def _find_subtask(self, task_id: str) -> Optional[SubTask]:
        """æŸ¥æ‰¾å­ä»»åŠ¡"""
        for st in self.plan.subtasks:
            if st.id == task_id:
                return st
        return None
    
    def execute_subtask(self, subtask: SubTask) -> Dict:
        """æ‰§è¡Œå•ä¸ªå­ä»»åŠ¡"""
        logger.info(f"Executing subtask: {subtask.id} - {subtask.title}")
        subtask.status = TaskStatus.RUNNING
        subtask.start_time = time.time()
        
        try:
            # æ ¹æ®ä»»åŠ¡ç±»å‹é€‰æ‹©å¤„ç†å™¨
            handler = self.task_handlers.get(subtask.task_type)
            if not handler:
                handler = self.task_handlers.get('default')
            
            if not handler:
                raise ValueError(f"No handler found for task type: {subtask.task_type}")
            
            # æ‰§è¡Œä»»åŠ¡
            result = handler(subtask)
            
            subtask.status = TaskStatus.COMPLETED
            subtask.result = result
            subtask.end_time = time.time()
            
            elapsed = subtask.end_time - subtask.start_time
            logger.info(f"âœ“ Completed subtask {subtask.id} in {elapsed:.1f}s")
            
            return result
            
        except Exception as e:
            subtask.status = TaskStatus.FAILED
            subtask.error = str(e)
            subtask.end_time = time.time()
            
            logger.error(f"âœ— Failed subtask {subtask.id}: {e}")
            
            if self.plan.fail_fast:
                raise
            
            return {'status': 'failed', 'error': str(e)}
    
    def execute_batch(self, batch: List[SubTask]) -> List[Dict]:
        """æ‰§è¡Œä¸€æ‰¹ä»»åŠ¡ï¼ˆå¹¶è¡Œæˆ–ä¸²è¡Œï¼‰"""
        if not self.plan.enable_parallel or len(batch) == 1:
            # ä¸²è¡Œæ‰§è¡Œ
            logger.info(f"Executing {len(batch)} tasks serially")
            return [self.execute_subtask(st) for st in batch]
        
        # å¹¶è¡Œæ‰§è¡Œ
        logger.info(f"Executing {len(batch)} tasks in parallel")
        futures = {
            self.executor.submit(self.execute_subtask, st): st
            for st in batch
        }
        
        results = []
        for future in as_completed(futures):
            try:
                result = future.result()
                results.append(result)
            except Exception as e:
                logger.error(f"Task execution error: {e}")
                if self.plan.fail_fast:
                    raise
                results.append({'status': 'failed', 'error': str(e)})
        
        return results
    
    def execute(self) -> Dict:
        """æ‰§è¡Œæ•´ä¸ªä»»åŠ¡è®¡åˆ’"""
        logger.info(f"=== Starting execution of plan: {self.plan.task_id} ===")
        logger.info(f"Total subtasks: {len(self.plan.subtasks)}")
        
        start_time = time.time()
        iteration = 0
        max_iterations = len(self.plan.subtasks) * 2  # é˜²æ­¢æ­»å¾ªç¯
        
        while iteration < max_iterations:
            iteration += 1
            
            # æ‰¾å‡ºæ‰€æœ‰å¯ä»¥æ‰§è¡Œçš„ä»»åŠ¡
            ready_tasks = [st for st in self.plan.subtasks if self.can_execute(st)]
            
            if not ready_tasks:
                # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰pendingä»»åŠ¡
                pending = [st for st in self.plan.subtasks if st.status == TaskStatus.PENDING]
                if pending:
                    logger.warning(f"{len(pending)} tasks are blocked by dependencies or failed")
                    for st in pending:
                        st.status = TaskStatus.SKIPPED
                        logger.warning(f"Skipped task: {st.id} - {st.title}")
                break
            
            # æŒ‰ä¼˜å…ˆçº§æ’åºï¼Œå–å‰Nä¸ª
            ready_tasks.sort(key=lambda x: (-x.priority, x.id))
            batch = ready_tasks[:self.plan.batch_size]
            
            logger.info(f"\n--- Batch {iteration}: {len(batch)} tasks ---")
            for st in batch:
                deps_str = f" (depends on: {', '.join(st.depends_on)})" if st.depends_on else ""
                logger.info(f"  â€¢ {st.id}: {st.title} [priority={st.priority}]{deps_str}")
            
            # æ‰§è¡Œæ‰¹æ¬¡
            self.execute_batch(batch)
            
            # æ£€æŸ¥æ˜¯å¦è¶…æ—¶
            elapsed = time.time() - start_time
            if elapsed > self.plan.max_execution_time_seconds:
                logger.warning(f"Execution timeout ({elapsed:.0f}s > {self.plan.max_execution_time_seconds}s)")
                break
        
        # ç»Ÿè®¡ç»“æœ
        completed = sum(1 for st in self.plan.subtasks if st.status == TaskStatus.COMPLETED)
        failed = sum(1 for st in self.plan.subtasks if st.status == TaskStatus.FAILED)
        skipped = sum(1 for st in self.plan.subtasks if st.status == TaskStatus.SKIPPED)
        
        elapsed = time.time() - start_time
        
        logger.info(f"\n=== Execution Summary ===")
        logger.info(f"Completed: {completed}/{len(self.plan.subtasks)}")
        logger.info(f"Failed: {failed}")
        logger.info(f"Skipped: {skipped}")
        logger.info(f"Total time: {elapsed:.1f}s")
        
        return {
            'status': 'completed' if failed == 0 else 'partial_success',
            'total': len(self.plan.subtasks),
            'completed': completed,
            'failed': failed,
            'skipped': skipped,
            'elapsed_seconds': elapsed,
            'iterations': iteration,
            'subtasks': [st.to_dict() for st in self.plan.subtasks]
        }
    
    def __enter__(self):
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.executor.shutdown(wait=True)


class TaskAggregator:
    """ä»»åŠ¡ç»“æœèšåˆå™¨"""
    
    def __init__(self, plan: TaskPlan, execution_result: Dict):
        self.plan = plan
        self.execution_result = execution_result
    
    def aggregate(self, aggregators: Dict[str, Callable]) -> Dict:
        """
        èšåˆç»“æœ
        
        Args:
            aggregators: ä»»åŠ¡ç±»å‹åˆ°èšåˆå‡½æ•°çš„æ˜ å°„
                        ä¾‹å¦‚: {'review': aggregate_review_results}
        """
        aggregated = {
            'execution_summary': self.execution_result,
            'task_info': {
                'task_id': self.plan.task_id,
                'task_type': self.plan.task_type,
                'title': self.plan.title
            },
            'results_by_type': {}
        }
        
        # æŒ‰ä»»åŠ¡ç±»å‹åˆ†ç»„
        tasks_by_type = {}
        for subtask_dict in self.execution_result['subtasks']:
            subtask = SubTask.from_dict(subtask_dict)
            task_type = subtask.task_type
            
            if task_type not in tasks_by_type:
                tasks_by_type[task_type] = []
            tasks_by_type[task_type].append(subtask)
        
        # å¯¹æ¯ç§ç±»å‹è°ƒç”¨å¯¹åº”çš„èšåˆå™¨
        for task_type, subtasks in tasks_by_type.items():
            aggregator = aggregators.get(task_type) or aggregators.get('default')
            if aggregator:
                try:
                    aggregated['results_by_type'][task_type] = aggregator(subtasks)
                except Exception as e:
                    logger.error(f"Aggregation failed for type {task_type}: {e}")
                    aggregated['results_by_type'][task_type] = {
                        'error': str(e),
                        'task_count': len(subtasks)
                    }
        
        return aggregated
    
    def generate_summary_markdown(self, aggregated_results: Dict) -> str:
        """ç”ŸæˆMarkdownæ ¼å¼çš„æ‘˜è¦"""
        exec_summary = aggregated_results['execution_summary']
        task_info = aggregated_results['task_info']
        
        md = f"""## ğŸ¤– ä»»åŠ¡æ‰§è¡ŒæŠ¥å‘Š

### åŸºæœ¬ä¿¡æ¯
- **ä»»åŠ¡ID**: `{task_info['task_id']}`
- **ä»»åŠ¡ç±»å‹**: {task_info['task_type']}
- **ä»»åŠ¡æ ‡é¢˜**: {task_info['title']}

### æ‰§è¡Œç»Ÿè®¡
- âœ… æˆåŠŸ: {exec_summary['completed']} ä¸ª
- âŒ å¤±è´¥: {exec_summary['failed']} ä¸ª
- â­ï¸  è·³è¿‡: {exec_summary['skipped']} ä¸ª
- ğŸ“Š æ€»è®¡: {exec_summary['total']} ä¸ªå­ä»»åŠ¡
- â±ï¸  è€—æ—¶: {exec_summary['elapsed_seconds']:.1f} ç§’

"""
        
        # æ·»åŠ å„ç±»å‹çš„è¯¦ç»†ç»“æœ
        if aggregated_results.get('results_by_type'):
            md += "### è¯¦ç»†ç»“æœ\n\n"
            for task_type, results in aggregated_results['results_by_type'].items():
                md += f"#### {task_type.upper()}\n"
                if isinstance(results, dict):
                    for key, value in results.items():
                        if key != 'details':  # è·³è¿‡è¯¦ç»†ä¿¡æ¯
                            md += f"- **{key}**: {value}\n"
                md += "\n"
        
        return md


if __name__ == '__main__':
    # ç¤ºä¾‹ç”¨æ³•
    print("Task Framework loaded. Use this module to build task orchestration systems.")
